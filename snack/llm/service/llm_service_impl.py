import os
from rag.embedder import get_embedding
from rag.faiss_index import search as faiss_search
from langchain_core.tracers import LangChainTracer
from langchain_openai import ChatOpenAI


class LLMServiceImpl:
    def __init__(self):
        self.tracer = LangChainTracer(project_name=os.getenv("LANGCHAIN_PROJECT"))
        self.llm = ChatOpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            streaming=True,
            temperature=0.7,
            callbacks=[self.tracer],
            model="gpt-3.5-turbo"
        )

    def get_streaming_openai_response(self, prompt: str):
        print(f"ğŸ” í”„ë¡¬í”„íŠ¸:\n{prompt}")

        try:
            # RAG ì ìš©
            embedding = get_embedding(prompt)
            related_restaurants = faiss_search(embedding, top_k=3)

            restaurant_info = "\n".join([
                f"- {r['name']} ({r['address']})" for r in related_restaurants
            ])
            prompt += f"\n\nğŸ“Œ [ì°¸ê³  ê°€ëŠ¥í•œ ì‹ë‹¹ ì •ë³´]\n{restaurant_info}"

            # Streaming ì‘ë‹µ generator ë°˜í™˜
            return self.llm.stream(prompt)

        except Exception as e:
            print(f"âŒ OpenAI ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
            return iter([])  # ë¹ˆ generator ë°˜í™˜
